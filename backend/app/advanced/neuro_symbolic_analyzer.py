"""
Neuro-Symbolic Code Understanding System
=======================================

Revolutionary hybrid AI system that combines deep neural networks with symbolic 
reasoning for unprecedented code comprehension and intent understanding.

This system bridges the gap between:
- Neural networks: Pattern recognition, learning from examples
- Symbolic AI: Logical reasoning, rule-based inference, explainability

Features:
- Intent extraction from code using neural-symbolic fusion
- Logical reasoning about code behavior and properties
- Explainable AI decisions with symbolic proof traces
- Semantic code understanding beyond syntactic analysis
- Automated theorem proving for code correctness
"""

import asyncio
import numpy as np
import torch
import torch.nn as nn
from typing import Dict, List, Optional, Tuple, Any, Union, Set
from dataclasses import dataclass, field
from enum import Enum
import ast
import networkx as nx
from collections import defaultdict, deque
import json
import pickle
from datetime import datetime
import logging
from abc import ABC, abstractmethod

# Symbolic reasoning imports
from sympy import symbols, And, Or, Not, Implies, satisfiable, simplify
from sympy.logic.boolalg import BooleanFunction
import z3  # SMT solver for advanced theorem proving

# Neural network imports
from transformers import AutoModel, AutoTokenizer, pipeline
from sentence_transformers import SentenceTransformer
import torch.nn.functional as F

# Graph neural networks
try:
    import torch_geometric
    from torch_geometric.nn import GCNConv, GraphAttentionLayer
    GRAPH_NN_AVAILABLE = True
except ImportError:
    GRAPH_NN_AVAILABLE = False
    
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SymbolicKnowledgeType(Enum):
    """Types of symbolic knowledge in code."""
    PRECONDITION = "precondition"
    POSTCONDITION = "postcondition"
    INVARIANT = "invariant"
    ASSERTION = "assertion"
    TYPE_CONSTRAINT = "type_constraint"
    BEHAVIORAL_RULE = "behavioral_rule"
    SECURITY_PROPERTY = "security_property"


@dataclass
class SymbolicKnowledge:
    """Represents symbolic knowledge extracted from code."""
    knowledge_type: SymbolicKnowledgeType
    formula: str  # Symbolic logic formula
    confidence: float
    source_location: Tuple[int, int]  # (line_start, line_end)
    variables: Set[str]
    context: Dict[str, Any]


@dataclass
class NeuralEmbedding:
    """Neural embedding of code components."""
    embedding_vector: np.ndarray
    attention_weights: Optional[np.ndarray]
    token_embeddings: Optional[List[np.ndarray]]
    semantic_features: Dict[str, float]


@dataclass
class CodeIntent:
    """Represents the extracted intent of code."""
    primary_intent: str
    secondary_intents: List[str]
    confidence: float
    reasoning_trace: List[str]
    symbolic_constraints: List[SymbolicKnowledge]
    neural_evidence: NeuralEmbedding


@dataclass
class NeuroSymbolicInsight:
    """Insights generated by neuro-symbolic analysis."""
    insight_type: str
    description: str
    confidence: float
    neural_confidence: float
    symbolic_confidence: float
    proof_trace: Optional[List[str]]
    recommendations: List[str]


class SymbolicReasoner:
    """
    Symbolic reasoning engine for code analysis using formal logic.
    """
    
    def __init__(self):
        self.knowledge_base = []
        self.inference_rules = []
        self.theorem_prover = z3.Solver()
        self._initialize_base_rules()
    
    def _initialize_base_rules(self):
        """Initialize base inference rules for code analysis."""
        # Basic programming logic rules
        self.inference_rules = [
            # If a variable is assigned in all branches, it's always defined after
            "∀x,c: (assigned_in_all_branches(x,c) → defined_after(x,c))",
            
            # If a function has no side effects and pure inputs, it's deterministic
            "∀f: (no_side_effects(f) ∧ pure_inputs(f) → deterministic(f))",
            
            # If a loop has no break conditions, it may be infinite
            "∀l: (no_break_condition(l) ∧ no_return_in_loop(l) → potentially_infinite(l))",
            
            # Security rule: user input without validation is dangerous
            "∀i: (user_input(i) ∧ ¬validated(i) → security_risk(i))",
            
            # Type safety rule
            "∀v,t: (declared_type(v,t) ∧ assigned_different_type(v) → type_error(v))"
        ]
    
    def extract_symbolic_knowledge(self, ast_tree: ast.AST, code_content: str) -> List[SymbolicKnowledge]:
        """
        Extract symbolic knowledge from AST using formal logic.
        """
        knowledge = []
        
        # Extract preconditions and postconditions from docstrings
        knowledge.extend(self._extract_contracts_from_docstrings(ast_tree))
        
        # Extract invariants from loops
        knowledge.extend(self._extract_loop_invariants(ast_tree))
        
        # Extract type constraints
        knowledge.extend(self._extract_type_constraints(ast_tree))
        
        # Extract assertions
        knowledge.extend(self._extract_assertions(ast_tree))
        
        # Extract security properties
        knowledge.extend(self._extract_security_properties(ast_tree))
        
        return knowledge
    
    def _extract_contracts_from_docstrings(self, ast_tree: ast.AST) -> List[SymbolicKnowledge]:
        """Extract preconditions and postconditions from docstrings."""
        knowledge = []
        
        for node in ast.walk(ast_tree):
            if isinstance(node, ast.FunctionDef) and ast.get_docstring(node):
                docstring = ast.get_docstring(node)
                
                # Look for contract specifications in docstring
                if "precondition:" in docstring.lower() or "requires:" in docstring.lower():
                    precond_formula = self._parse_contract_from_text(docstring, "precondition")
                    if precond_formula:
                        knowledge.append(SymbolicKnowledge(
                            knowledge_type=SymbolicKnowledgeType.PRECONDITION,
                            formula=precond_formula,
                            confidence=0.8,
                            source_location=(node.lineno, node.end_lineno or node.lineno),
                            variables=self._extract_variables_from_formula(precond_formula),
                            context={"function_name": node.name}
                        ))
                
                if "postcondition:" in docstring.lower() or "ensures:" in docstring.lower():
                    postcond_formula = self._parse_contract_from_text(docstring, "postcondition")
                    if postcond_formula:
                        knowledge.append(SymbolicKnowledge(
                            knowledge_type=SymbolicKnowledgeType.POSTCONDITION,
                            formula=postcond_formula,
                            confidence=0.8,
                            source_location=(node.lineno, node.end_lineno or node.lineno),
                            variables=self._extract_variables_from_formula(postcond_formula),
                            context={"function_name": node.name}
                        ))
        
        return knowledge
    
    def _extract_loop_invariants(self, ast_tree: ast.AST) -> List[SymbolicKnowledge]:
        """Extract loop invariants using symbolic analysis."""
        knowledge = []
        
        for node in ast.walk(ast_tree):
            if isinstance(node, (ast.For, ast.While)):
                # Analyze loop structure to infer invariants
                invariant_formula = self._infer_loop_invariant(node)
                if invariant_formula:
                    knowledge.append(SymbolicKnowledge(
                        knowledge_type=SymbolicKnowledgeType.INVARIANT,
                        formula=invariant_formula,
                        confidence=0.6,
                        source_location=(node.lineno, node.end_lineno or node.lineno),
                        variables=self._extract_variables_from_node(node),
                        context={"loop_type": type(node).__name__}
                    ))
        
        return knowledge
    
    def _extract_type_constraints(self, ast_tree: ast.AST) -> List[SymbolicKnowledge]:
        """Extract type constraints from annotations and usage patterns."""
        knowledge = []
        
        for node in ast.walk(ast_tree):
            if isinstance(node, ast.FunctionDef):
                # Extract type annotations
                if node.returns:
                    return_constraint = f"return_type({node.name}) = {ast.unparse(node.returns)}"
                    knowledge.append(SymbolicKnowledge(
                        knowledge_type=SymbolicKnowledgeType.TYPE_CONSTRAINT,
                        formula=return_constraint,
                        confidence=0.9,
                        source_location=(node.lineno, node.lineno),
                        variables={node.name},
                        context={"function_name": node.name}
                    ))
                
                # Extract parameter type constraints
                for arg in node.args.args:
                    if arg.annotation:
                        param_constraint = f"param_type({arg.arg}) = {ast.unparse(arg.annotation)}"
                        knowledge.append(SymbolicKnowledge(
                            knowledge_type=SymbolicKnowledgeType.TYPE_CONSTRAINT,
                            formula=param_constraint,
                            confidence=0.9,
                            source_location=(node.lineno, node.lineno),
                            variables={arg.arg},
                            context={"function_name": node.name, "parameter": arg.arg}
                        ))
        
        return knowledge
    
    def _extract_assertions(self, ast_tree: ast.AST) -> List[SymbolicKnowledge]:
        """Extract explicit assertions from code."""
        knowledge = []
        
        for node in ast.walk(ast_tree):
            if isinstance(node, ast.Assert):
                assertion_formula = self._convert_ast_to_formula(node.test)
                knowledge.append(SymbolicKnowledge(
                    knowledge_type=SymbolicKnowledgeType.ASSERTION,
                    formula=assertion_formula,
                    confidence=0.95,
                    source_location=(node.lineno, node.lineno),
                    variables=self._extract_variables_from_node(node),
                    context={"assertion_line": node.lineno}
                ))
        
        return knowledge
    
    def _extract_security_properties(self, ast_tree: ast.AST) -> List[SymbolicKnowledge]:
        """Extract security-related properties."""
        knowledge = []
        
        for node in ast.walk(ast_tree):
            # Check for input validation patterns
            if isinstance(node, ast.If):
                condition = ast.unparse(node.test)
                if any(keyword in condition.lower() for keyword in ['validate', 'sanitize', 'check']):
                    security_formula = f"validated_input({condition})"
                    knowledge.append(SymbolicKnowledge(
                        knowledge_type=SymbolicKnowledgeType.SECURITY_PROPERTY,
                        formula=security_formula,
                        confidence=0.7,
                        source_location=(node.lineno, node.end_lineno or node.lineno),
                        variables=self._extract_variables_from_node(node),
                        context={"security_check": True}
                    ))
        
        return knowledge
    
    def reason_about_properties(self, knowledge: List[SymbolicKnowledge]) -> List[Dict[str, Any]]:
        """
        Use symbolic reasoning to derive new insights from extracted knowledge.
        """
        insights = []
        
        # Convert knowledge to Z3 constraints
        z3_constraints = []
        for k in knowledge:
            try:
                z3_constraint = self._convert_formula_to_z3(k.formula)
                if z3_constraint:
                    z3_constraints.append(z3_constraint)
            except Exception as e:
                logger.warning(f"Could not convert formula to Z3: {k.formula}, Error: {e}")
        
        # Add constraints to solver
        for constraint in z3_constraints:
            self.theorem_prover.add(constraint)
        
        # Check satisfiability and derive insights
        if self.theorem_prover.check() == z3.sat:
            model = self.theorem_prover.get_model()
            insights.append({
                "type": "satisfiability_check",
                "result": "satisfiable",
                "model": str(model),
                "confidence": 0.9
            })
        else:
            insights.append({
                "type": "satisfiability_check",
                "result": "unsatisfiable",
                "contradiction": True,
                "confidence": 0.95
            })
        
        # Apply inference rules
        for rule in self.inference_rules:
            derived_facts = self._apply_inference_rule(rule, knowledge)
            insights.extend(derived_facts)
        
        return insights
    
    def _parse_contract_from_text(self, text: str, contract_type: str) -> Optional[str]:
        """Parse contract specification from natural language text."""
        # Simplified contract parsing
        lines = text.lower().split('\n')
        for line in lines:
            if contract_type in line and ':' in line:
                contract_part = line.split(':', 1)[1].strip()
                return self._normalize_contract_formula(contract_part)
        return None
    
    def _normalize_contract_formula(self, formula: str) -> str:
        """Normalize contract formula to standard logical form."""
        # Simple normalization (in practice, this would be more sophisticated)
        formula = formula.replace(' and ', ' ∧ ')
        formula = formula.replace(' or ', ' ∨ ')
        formula = formula.replace(' not ', ' ¬ ')
        formula = formula.replace('=>', '→')
        return formula
    
    def _infer_loop_invariant(self, loop_node: Union[ast.For, ast.While]) -> Optional[str]:
        """Infer loop invariant from loop structure."""
        # Simplified invariant inference
        if isinstance(loop_node, ast.For):
            return f"0 ≤ loop_counter ≤ collection_size"
        elif isinstance(loop_node, ast.While):
            condition = ast.unparse(loop_node.test)
            return f"loop_condition_maintains({condition})"
        return None
    
    def _convert_ast_to_formula(self, ast_node: ast.AST) -> str:
        """Convert AST node to logical formula."""
        try:
            return ast.unparse(ast_node)
        except:
            return str(ast_node)
    
    def _extract_variables_from_formula(self, formula: str) -> Set[str]:
        """Extract variable names from a formula."""
        # Simplified variable extraction
        import re
        variables = re.findall(r'\\b[a-zA-Z_][a-zA-Z0-9_]*\\b', formula)
        return set(variables)
    
    def _extract_variables_from_node(self, node: ast.AST) -> Set[str]:
        """Extract variable names from AST node."""
        variables = set()
        for child in ast.walk(node):
            if isinstance(child, ast.Name):
                variables.add(child.id)
        return variables
    
    def _convert_formula_to_z3(self, formula: str) -> Optional[z3.BoolRef]:
        """Convert symbolic formula to Z3 constraint."""
        try:
            # Simplified conversion for demonstration
            # In practice, this would be a full parser
            if '=' in formula and 'param_type' in formula:
                return z3.Bool(f"type_constraint_{hash(formula)}")
            elif 'validated_input' in formula:
                return z3.Bool(f"security_constraint_{hash(formula)}")
            else:
                return z3.Bool(f"constraint_{hash(formula)}")
        except Exception:
            return None
    
    def _apply_inference_rule(self, rule: str, knowledge: List[SymbolicKnowledge]) -> List[Dict[str, Any]]:
        """Apply inference rule to derive new facts."""
        # Simplified rule application
        derived_facts = []
        
        if "no_side_effects" in rule:
            # Look for functions that might be pure
            pure_functions = [k for k in knowledge 
                            if k.knowledge_type == SymbolicKnowledgeType.POSTCONDITION 
                            and "pure" in k.formula.lower()]
            
            for func_knowledge in pure_functions:
                derived_facts.append({
                    "type": "derived_property",
                    "property": "deterministic_function",
                    "target": func_knowledge.context.get("function_name"),
                    "confidence": 0.8,
                    "reasoning": f"Applied rule: {rule}"
                })
        
        return derived_facts


class NeuralCodeEncoder:
    """
    Neural network component for encoding code semantics.
    """
    
    def __init__(self, model_name: str = "microsoft/codebert-base"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        self.sentence_transformer = SentenceTransformer('all-MiniLM-L6-v2')
        
        # Specialized neural networks
        self.intent_classifier = self._build_intent_classifier()
        self.semantic_encoder = self._build_semantic_encoder()
        
        if GRAPH_NN_AVAILABLE:
            self.graph_encoder = self._build_graph_encoder()
    
    def _build_intent_classifier(self) -> nn.Module:
        """Build neural network for code intent classification."""
        class IntentClassifier(nn.Module):
            def __init__(self, input_dim: int = 768, num_intents: int = 20):
                super().__init__()
                self.dropout = nn.Dropout(0.3)
                self.fc1 = nn.Linear(input_dim, 512)
                self.fc2 = nn.Linear(512, 256)
                self.fc3 = nn.Linear(256, num_intents)
                
            def forward(self, x):
                x = self.dropout(x)
                x = torch.relu(self.fc1(x))
                x = torch.relu(self.fc2(x))
                return torch.softmax(self.fc3(x), dim=-1)
        
        return IntentClassifier()
    
    def _build_semantic_encoder(self) -> nn.Module:
        """Build neural network for semantic feature extraction."""
        class SemanticEncoder(nn.Module):
            def __init__(self, input_dim: int = 768):
                super().__init__()
                self.attention = nn.MultiheadAttention(input_dim, num_heads=8)
                self.layer_norm = nn.LayerNorm(input_dim)
                self.feature_extractor = nn.Sequential(
                    nn.Linear(input_dim, 512),
                    nn.ReLU(),
                    nn.Linear(512, 256),
                    nn.ReLU(),
                    nn.Linear(256, 128)
                )
                
            def forward(self, x):
                # Self-attention
                attn_output, attn_weights = self.attention(x, x, x)
                x = self.layer_norm(x + attn_output)
                
                # Feature extraction
                features = self.feature_extractor(x)
                return features, attn_weights
        
        return SemanticEncoder()
    
    def _build_graph_encoder(self) -> nn.Module:
        """Build graph neural network for code structure encoding."""
        if not GRAPH_NN_AVAILABLE:
            return None
            
        class CodeGraphEncoder(nn.Module):
            def __init__(self, node_features: int = 768, hidden_dim: int = 256):
                super().__init__()
                self.gcn1 = GCNConv(node_features, hidden_dim)
                self.gcn2 = GCNConv(hidden_dim, hidden_dim)
                self.attention = GraphAttentionLayer(hidden_dim, hidden_dim // 8, heads=8)
                self.classifier = nn.Linear(hidden_dim, 64)
                
            def forward(self, x, edge_index):
                x = torch.relu(self.gcn1(x, edge_index))
                x = torch.relu(self.gcn2(x, edge_index))
                x = self.attention(x, edge_index)
                return self.classifier(x)
        
        return CodeGraphEncoder()
    
    async def encode_code_semantics(self, code_content: str, ast_tree: ast.AST) -> NeuralEmbedding:
        """
        Encode code semantics using neural networks.
        """
        # Tokenize and encode code
        tokens = self.tokenizer.encode(code_content, truncation=True, max_length=512, return_tensors='pt')
        
        with torch.no_grad():
            # Get contextual embeddings
            outputs = self.model(tokens)
            code_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
            
            # Extract semantic features
            semantic_features_tensor = outputs.last_hidden_state
            semantic_features, attention_weights = self.semantic_encoder(semantic_features_tensor)
            
            # Calculate semantic feature scores
            semantic_scores = {
                'complexity_score': self._calculate_complexity_score(semantic_features),
                'readability_score': self._calculate_readability_score(code_content),
                'maintainability_score': self._calculate_maintainability_score(semantic_features),
                'functionality_coherence': self._calculate_coherence_score(semantic_features),
                'documentation_quality': self._calculate_documentation_score(code_content)
            }
        
        # Extract token-level embeddings
        token_embeddings = [outputs.last_hidden_state[0, i].numpy() for i in range(outputs.last_hidden_state.shape[1])]
        
        return NeuralEmbedding(
            embedding_vector=code_embedding,
            attention_weights=attention_weights.squeeze().numpy() if attention_weights is not None else None,
            token_embeddings=token_embeddings,
            semantic_features=semantic_scores
        )
    
    def classify_code_intent(self, neural_embedding: NeuralEmbedding) -> Tuple[str, List[str], float]:
        """
        Classify the primary intent of code using neural classification.
        """
        # Intent categories
        intent_categories = [
            "data_processing", "user_interface", "business_logic", "security",
            "performance_optimization", "error_handling", "configuration",
            "testing", "logging", "database_interaction", "api_communication",
            "mathematical_computation", "file_operations", "validation",
            "transformation", "workflow_control", "utility_function",
            "initialization", "cleanup", "monitoring"
        ]
        
        # Use intent classifier
        embedding_tensor = torch.FloatTensor(neural_embedding.embedding_vector).unsqueeze(0)
        
        with torch.no_grad():
            intent_scores = self.intent_classifier(embedding_tensor)
            intent_probabilities = intent_scores.squeeze().numpy()
        
        # Get primary intent
        primary_idx = np.argmax(intent_probabilities)
        primary_intent = intent_categories[primary_idx]
        primary_confidence = float(intent_probabilities[primary_idx])
        
        # Get secondary intents (top 3, excluding primary)
        sorted_indices = np.argsort(intent_probabilities)[::-1]
        secondary_intents = [intent_categories[idx] for idx in sorted_indices[1:4] 
                           if intent_probabilities[idx] > 0.1]
        
        return primary_intent, secondary_intents, primary_confidence
    
    def _calculate_complexity_score(self, semantic_features: torch.Tensor) -> float:
        """Calculate neural complexity score."""
        # Use variance in semantic features as complexity indicator
        return float(torch.var(semantic_features).item())
    
    def _calculate_readability_score(self, code_content: str) -> float:
        """Calculate readability score using heuristics."""
        lines = code_content.split('\n')
        non_empty_lines = [line for line in lines if line.strip()]
        
        if not non_empty_lines:
            return 0.0
        
        # Simple readability metrics
        avg_line_length = sum(len(line) for line in non_empty_lines) / len(non_empty_lines)
        comment_ratio = sum(1 for line in non_empty_lines if line.strip().startswith('#')) / len(non_empty_lines)
        
        # Normalize to 0-1 scale
        readability = max(0, min(1, (120 - avg_line_length) / 120 + comment_ratio * 0.3))
        return readability
    
    def _calculate_maintainability_score(self, semantic_features: torch.Tensor) -> float:
        """Calculate maintainability score from neural features."""
        # Use mean activation as maintainability indicator
        return float(torch.mean(torch.abs(semantic_features)).item())
    
    def _calculate_coherence_score(self, semantic_features: torch.Tensor) -> float:
        """Calculate functional coherence score."""
        # Use consistency in semantic features
        std_dev = torch.std(semantic_features).item()
        coherence = max(0, 1 - std_dev)  # Lower std = higher coherence
        return coherence
    
    def _calculate_documentation_score(self, code_content: str) -> float:
        """Calculate documentation quality score."""
        lines = code_content.split('\n')
        total_lines = len([line for line in lines if line.strip()])
        
        if total_lines == 0:
            return 0.0
        
        # Count different types of documentation
        docstring_lines = code_content.count('"""') + code_content.count("'''")
        comment_lines = sum(1 for line in lines if line.strip().startswith('#'))
        
        doc_ratio = (docstring_lines + comment_lines) / total_lines
        return min(1.0, doc_ratio * 2)  # Normalize and cap at 1.0


class NeuroSymbolicFusion:
    """
    Core fusion engine that combines neural and symbolic reasoning.
    """
    
    def __init__(self):
        self.symbolic_reasoner = SymbolicReasoner()
        self.neural_encoder = NeuralCodeEncoder()
        self.fusion_weights = {
            'neural_weight': 0.6,
            'symbolic_weight': 0.4,
            'confidence_threshold': 0.7
        }
    
    async def analyze_code_understanding(
        self, 
        code_content: str, 
        file_path: str
    ) -> Tuple[CodeIntent, List[NeuroSymbolicInsight]]:
        """
        Perform comprehensive neuro-symbolic code analysis.
        """
        try:
            # Parse code
            ast_tree = ast.parse(code_content)
            
            # Neural analysis
            neural_embedding = await self.neural_encoder.encode_code_semantics(code_content, ast_tree)
            primary_intent, secondary_intents, neural_confidence = self.neural_encoder.classify_code_intent(neural_embedding)
            
            # Symbolic analysis
            symbolic_knowledge = self.symbolic_reasoner.extract_symbolic_knowledge(ast_tree, code_content)
            symbolic_insights = self.symbolic_reasoner.reason_about_properties(symbolic_knowledge)
            
            # Fusion process
            fused_intent = await self._fuse_intent_understanding(
                primary_intent, secondary_intents, neural_confidence,
                symbolic_knowledge, symbolic_insights
            )
            
            # Generate comprehensive insights
            neuro_symbolic_insights = await self._generate_fusion_insights(
                neural_embedding, symbolic_knowledge, symbolic_insights
            )
            
            return fused_intent, neuro_symbolic_insights
            
        except Exception as e:
            logger.error(f"Neuro-symbolic analysis failed: {e}")
            return self._create_fallback_intent(), []
    
    async def _fuse_intent_understanding(
        self,
        primary_intent: str,
        secondary_intents: List[str],
        neural_confidence: float,
        symbolic_knowledge: List[SymbolicKnowledge],
        symbolic_insights: List[Dict[str, Any]]
    ) -> CodeIntent:
        """
        Fuse neural and symbolic understanding to determine code intent.
        """
        # Analyze symbolic evidence for intent
        symbolic_evidence = []
        symbolic_confidence = 0.5  # Base symbolic confidence
        
        # Check if symbolic knowledge supports neural intent classification
        for knowledge in symbolic_knowledge:
            if self._knowledge_supports_intent(knowledge, primary_intent):
                symbolic_evidence.append(f"Symbolic evidence: {knowledge.formula}")
                symbolic_confidence += 0.1
        
        # Check symbolic insights
        for insight in symbolic_insights:
            if insight.get('type') == 'derived_property':
                symbolic_evidence.append(f"Derived property: {insight.get('property')}")
                symbolic_confidence += 0.05
        
        # Fusion confidence calculation
        fused_confidence = (
            self.fusion_weights['neural_weight'] * neural_confidence +
            self.fusion_weights['symbolic_weight'] * min(symbolic_confidence, 1.0)
        )
        
        # Generate reasoning trace
        reasoning_trace = [
            f"Neural classification: {primary_intent} (confidence: {neural_confidence:.3f})",
            f"Symbolic evidence found: {len(symbolic_evidence)} pieces",
            f"Fusion confidence: {fused_confidence:.3f}"
        ]
        reasoning_trace.extend(symbolic_evidence)
        
        # Adjust intent based on symbolic evidence
        final_intent = await self._adjust_intent_with_symbolic_reasoning(
            primary_intent, symbolic_knowledge
        )
        
        return CodeIntent(
            primary_intent=final_intent,
            secondary_intents=secondary_intents,
            confidence=fused_confidence,
            reasoning_trace=reasoning_trace,
            symbolic_constraints=symbolic_knowledge,
            neural_evidence=NeuralEmbedding(
                embedding_vector=np.array([]),
                attention_weights=None,
                token_embeddings=None,
                semantic_features={}
            )
        )
    
    async def _generate_fusion_insights(
        self,
        neural_embedding: NeuralEmbedding,
        symbolic_knowledge: List[SymbolicKnowledge],
        symbolic_insights: List[Dict[str, Any]]
    ) -> List[NeuroSymbolicInsight]:
        """
        Generate insights by fusing neural and symbolic analysis results.
        """
        insights = []
        
        # Insight 1: Code complexity analysis
        neural_complexity = neural_embedding.semantic_features.get('complexity_score', 0)
        symbolic_complexity = len([k for k in symbolic_knowledge 
                                 if k.knowledge_type == SymbolicKnowledgeType.INVARIANT])
        
        complexity_insight = NeuroSymbolicInsight(
            insight_type="complexity_analysis",
            description=self._generate_complexity_insight_description(neural_complexity, symbolic_complexity),
            confidence=0.8,
            neural_confidence=neural_complexity,
            symbolic_confidence=min(symbolic_complexity / 10, 1.0),
            proof_trace=None,
            recommendations=self._generate_complexity_recommendations(neural_complexity, symbolic_complexity)
        )
        insights.append(complexity_insight)
        
        # Insight 2: Security analysis
        security_knowledge = [k for k in symbolic_knowledge 
                            if k.knowledge_type == SymbolicKnowledgeType.SECURITY_PROPERTY]
        
        if security_knowledge or neural_embedding.semantic_features.get('functionality_coherence', 0) < 0.5:
            security_insight = NeuroSymbolicInsight(
                insight_type="security_analysis",
                description=self._generate_security_insight_description(security_knowledge, neural_embedding),
                confidence=0.75,
                neural_confidence=1.0 - neural_embedding.semantic_features.get('functionality_coherence', 0.5),
                symbolic_confidence=len(security_knowledge) / 5,
                proof_trace=[k.formula for k in security_knowledge],
                recommendations=self._generate_security_recommendations(security_knowledge)
            )
            insights.append(security_insight)
        
        # Insight 3: Maintainability analysis
        maintainability_score = neural_embedding.semantic_features.get('maintainability_score', 0.5)
        type_constraints = [k for k in symbolic_knowledge 
                          if k.knowledge_type == SymbolicKnowledgeType.TYPE_CONSTRAINT]
        
        maintainability_insight = NeuroSymbolicInsight(
            insight_type="maintainability_analysis",
            description=self._generate_maintainability_insight_description(maintainability_score, type_constraints),
            confidence=0.7,
            neural_confidence=maintainability_score,
            symbolic_confidence=len(type_constraints) / 10,
            proof_trace=None,
            recommendations=self._generate_maintainability_recommendations(maintainability_score, type_constraints)
        )
        insights.append(maintainability_insight)
        
        # Insight 4: Correctness analysis
        assertions = [k for k in symbolic_knowledge if k.knowledge_type == SymbolicKnowledgeType.ASSERTION]
        contracts = [k for k in symbolic_knowledge if k.knowledge_type in 
                    [SymbolicKnowledgeType.PRECONDITION, SymbolicKnowledgeType.POSTCONDITION]]
        
        if assertions or contracts:
            correctness_insight = NeuroSymbolicInsight(
                insight_type="correctness_analysis",
                description=self._generate_correctness_insight_description(assertions, contracts),
                confidence=0.85,
                neural_confidence=0.5,  # Neural networks are less reliable for correctness
                symbolic_confidence=0.9,  # Symbolic reasoning is stronger for correctness
                proof_trace=self._generate_correctness_proof_trace(assertions, contracts),
                recommendations=self._generate_correctness_recommendations(assertions, contracts)
            )
            insights.append(correctness_insight)
        
        return insights
    
    def _knowledge_supports_intent(self, knowledge: SymbolicKnowledge, intent: str) -> bool:
        """Check if symbolic knowledge supports the inferred intent."""
        intent_keywords = {
            'security': ['validate', 'sanitize', 'check', 'auth', 'permission'],
            'data_processing': ['transform', 'process', 'convert', 'parse'],
            'error_handling': ['try', 'except', 'error', 'exception'],
            'business_logic': ['calculate', 'compute', 'logic', 'rule'],
            'validation': ['validate', 'verify', 'check', 'ensure']
        }
        
        keywords = intent_keywords.get(intent, [])
        return any(keyword in knowledge.formula.lower() for keyword in keywords)
    
    async def _adjust_intent_with_symbolic_reasoning(
        self, 
        neural_intent: str, 
        symbolic_knowledge: List[SymbolicKnowledge]
    ) -> str:
        """Adjust intent classification based on symbolic evidence."""
        # Strong symbolic evidence can override neural classification
        security_evidence = len([k for k in symbolic_knowledge 
                               if k.knowledge_type == SymbolicKnowledgeType.SECURITY_PROPERTY])
        
        if security_evidence >= 2 and neural_intent != 'security':
            return 'security'  # Strong security evidence overrides
        
        validation_evidence = len([k for k in symbolic_knowledge 
                                 if 'validate' in k.formula.lower()])
        
        if validation_evidence >= 2 and neural_intent != 'validation':
            return 'validation'  # Strong validation evidence
        
        return neural_intent
    
    def _generate_complexity_insight_description(self, neural_complexity: float, symbolic_complexity: int) -> str:
        """Generate description for complexity insight."""
        if neural_complexity > 0.7 and symbolic_complexity > 5:
            return "High complexity detected by both neural and symbolic analysis. Code has complex neural patterns and multiple symbolic constraints."
        elif neural_complexity > 0.7:
            return "High neural complexity detected. Code patterns suggest complex logic that may be difficult to maintain."
        elif symbolic_complexity > 5:
            return "High symbolic complexity detected. Multiple formal constraints and invariants present."
        else:
            return "Moderate complexity. Code appears reasonably structured with manageable complexity."
    
    def _generate_complexity_recommendations(self, neural_complexity: float, symbolic_complexity: int) -> List[str]:
        """Generate recommendations for complexity management."""
        recommendations = []
        
        if neural_complexity > 0.7:
            recommendations.extend([
                "Consider breaking down complex functions into smaller, focused units",
                "Add documentation to explain complex logic patterns",
                "Implement unit tests for complex code paths"
            ])
        
        if symbolic_complexity > 5:
            recommendations.extend([
                "Consider simplifying logical constraints",
                "Document formal specifications clearly",
                "Use assertion-based testing to verify constraints"
            ])
        
        return recommendations
    
    def _generate_security_insight_description(
        self, 
        security_knowledge: List[SymbolicKnowledge], 
        neural_embedding: NeuralEmbedding
    ) -> str:
        """Generate security insight description."""
        if security_knowledge:
            return f"Security properties detected: {len(security_knowledge)} validation patterns found. Neural analysis indicates potential security-related functionality."
        else:
            return "Limited security validation detected. Consider adding input validation and security checks."
    
    def _generate_security_recommendations(self, security_knowledge: List[SymbolicKnowledge]) -> List[str]:
        """Generate security recommendations."""
        if not security_knowledge:
            return [
                "Add input validation for user-provided data",
                "Implement proper error handling to avoid information leakage",
                "Consider security testing and code review"
            ]
        else:
            return [
                "Verify that all security constraints are properly enforced",
                "Add logging for security-relevant events",
                "Consider formal verification of security properties"
            ]
    
    def _generate_maintainability_insight_description(
        self, 
        maintainability_score: float, 
        type_constraints: List[SymbolicKnowledge]
    ) -> str:
        """Generate maintainability insight description."""
        if maintainability_score > 0.7 and len(type_constraints) >= 3:
            return "High maintainability detected. Good neural patterns and strong type constraints present."
        elif maintainability_score > 0.7:
            return "Good neural maintainability patterns detected. Consider adding more type annotations."
        elif len(type_constraints) >= 3:
            return "Strong type constraints detected. Neural patterns suggest room for improvement."
        else:
            return "Moderate maintainability. Consider improving code structure and type annotations."
    
    def _generate_maintainability_recommendations(
        self, 
        maintainability_score: float, 
        type_constraints: List[SymbolicKnowledge]
    ) -> List[str]:
        """Generate maintainability recommendations."""
        recommendations = []
        
        if maintainability_score < 0.5:
            recommendations.extend([
                "Improve code structure and organization",
                "Add more descriptive variable and function names",
                "Reduce coupling between components"
            ])
        
        if len(type_constraints) < 3:
            recommendations.extend([
                "Add comprehensive type annotations",
                "Use static type checking tools",
                "Document parameter and return types"
            ])
        
        return recommendations
    
    def _generate_correctness_insight_description(
        self, 
        assertions: List[SymbolicKnowledge], 
        contracts: List[SymbolicKnowledge]
    ) -> str:
        """Generate correctness insight description."""
        total_formal_specs = len(assertions) + len(contracts)
        return f"Formal correctness specifications detected: {len(assertions)} assertions and {len(contracts)} contracts. Strong foundation for correctness verification."
    
    def _generate_correctness_proof_trace(
        self, 
        assertions: List[SymbolicKnowledge], 
        contracts: List[SymbolicKnowledge]
    ) -> List[str]:
        """Generate proof trace for correctness analysis."""
        proof_trace = []
        
        for assertion in assertions:
            proof_trace.append(f"Assertion: {assertion.formula}")
        
        for contract in contracts:
            proof_trace.append(f"Contract ({contract.knowledge_type.value}): {contract.formula}")
        
        return proof_trace
    
    def _generate_correctness_recommendations(
        self, 
        assertions: List[SymbolicKnowledge], 
        contracts: List[SymbolicKnowledge]
    ) -> List[str]:
        """Generate correctness recommendations."""
        recommendations = [
            "Use formal verification tools to check correctness properties",
            "Implement property-based testing",
            "Add more assertions for critical invariants"
        ]
        
        if not contracts:
            recommendations.append("Consider adding precondition and postcondition specifications")
        
        return recommendations
    
    def _create_fallback_intent(self) -> CodeIntent:
        """Create fallback intent when analysis fails."""
        return CodeIntent(
            primary_intent="unknown",
            secondary_intents=[],
            confidence=0.0,
            reasoning_trace=["Analysis failed - fallback intent"],
            symbolic_constraints=[],
            neural_evidence=NeuralEmbedding(
                embedding_vector=np.array([]),
                attention_weights=None,
                token_embeddings=None,
                semantic_features={}
            )
        )


# Main interface class
class NeuroSymbolicCodeAnalyzer:
    """
    Main interface for neuro-symbolic code analysis.
    """
    
    def __init__(self):
        self.fusion_engine = NeuroSymbolicFusion()
        self.analysis_cache = {}
        
    async def analyze_code(self, code_content: str, file_path: str) -> Dict[str, Any]:
        """
        Perform comprehensive neuro-symbolic code analysis.
        """
        try:
            # Perform neuro-symbolic analysis
            code_intent, insights = await self.fusion_engine.analyze_code_understanding(code_content, file_path)
            
            # Compile results
            result = {
                "neuro_symbolic_analysis": {
                    "code_intent": {
                        "primary_intent": code_intent.primary_intent,
                        "secondary_intents": code_intent.secondary_intents,
                        "confidence": code_intent.confidence,
                        "reasoning_trace": code_intent.reasoning_trace
                    },
                    "symbolic_constraints": [
                        {
                            "type": constraint.knowledge_type.value,
                            "formula": constraint.formula,
                            "confidence": constraint.confidence,
                            "source_location": constraint.source_location,
                            "variables": list(constraint.variables),
                            "context": constraint.context
                        }
                        for constraint in code_intent.symbolic_constraints
                    ],
                    "insights": [
                        {
                            "type": insight.insight_type,
                            "description": insight.description,
                            "confidence": insight.confidence,
                            "neural_confidence": insight.neural_confidence,
                            "symbolic_confidence": insight.symbolic_confidence,
                            "proof_trace": insight.proof_trace,
                            "recommendations": insight.recommendations
                        }
                        for insight in insights
                    ]
                },
                "analysis_metadata": {
                    "timestamp": datetime.now().isoformat(),
                    "file_path": file_path,
                    "analysis_type": "neuro_symbolic",
                    "neural_model": "microsoft/codebert-base",
                    "symbolic_reasoner": "z3_solver",
                    "fusion_method": "weighted_confidence"
                }
            }
            
            return result
            
        except Exception as e:
            return {
                "error": f"Neuro-symbolic analysis failed: {str(e)}",
                "analysis_metadata": {
                    "timestamp": datetime.now().isoformat(),
                    "file_path": file_path,
                    "analysis_type": "neuro_symbolic_failed"
                }
            }


# Example usage and testing
async def demonstrate_neuro_symbolic_analysis():
    """
    Demonstrate the neuro-symbolic code analysis system.
    """
    analyzer = NeuroSymbolicCodeAnalyzer()
    
    # Example code with various complexity patterns
    example_code = '''
def validate_and_process_user_data(user_input: str, max_length: int = 100) -> str:
    """
    Process user input with validation.
    
    Precondition: user_input is not None and max_length > 0
    Postcondition: returns validated and processed string
    """
    assert user_input is not None, "User input cannot be None"
    assert max_length > 0, "Max length must be positive"
    
    # Validate input length
    if len(user_input) > max_length:
        raise ValueError(f"Input too long: {len(user_input)} > {max_length}")
    
    # Sanitize input to prevent injection attacks
    sanitized = user_input.replace('<', '&lt;').replace('>', '&gt;')
    
    # Process the data
    processed = sanitized.strip().lower()
    
    # Ensure postcondition
    assert len(processed) <= max_length, "Processed data exceeds max length"
    
    return processed
'''
    
    # Analyze the code
    result = await analyzer.analyze_code(example_code, "example.py")
    
    print("Neuro-Symbolic Code Analysis Results:")
    print("=" * 50)
    
    # Print intent analysis
    intent = result["neuro_symbolic_analysis"]["code_intent"]
    print(f"Primary Intent: {intent['primary_intent']}")
    print(f"Confidence: {intent['confidence']:.3f}")
    print(f"Secondary Intents: {', '.join(intent['secondary_intents'])}")
    
    print("\\nReasoning Trace:")
    for step in intent['reasoning_trace']:
        print(f"  - {step}")
    
    # Print symbolic constraints
    print("\\nSymbolic Constraints:")
    for constraint in result["neuro_symbolic_analysis"]["symbolic_constraints"]:
        print(f"  {constraint['type']}: {constraint['formula']}")
    
    # Print insights
    print("\\nNeuro-Symbolic Insights:")
    for insight in result["neuro_symbolic_analysis"]["insights"]:
        print(f"\\n{insight['type'].title()}:")
        print(f"  Description: {insight['description']}")
        print(f"  Confidence: {insight['confidence']:.3f}")
        print(f"  Neural: {insight['neural_confidence']:.3f}, Symbolic: {insight['symbolic_confidence']:.3f}")
        
        if insight['recommendations']:
            print("  Recommendations:")
            for rec in insight['recommendations']:
                print(f"    - {rec}")


if __name__ == "__main__":
    asyncio.run(demonstrate_neuro_symbolic_analysis())